import streamlit as st
import pandas as pd
import numpy as np
import sys
sys.path.append('..')
from utils.db_loader import get_db_connection
from utils.scale_info import get_scale_info, SCALE_DESCRIPTIONS
from utils.statistical_analysis import correlation_with_pvalue
from pathlib import Path
from io import BytesIO

# ============================================
# PAGE CONFIG
# ============================================

st.set_page_config(
    page_title="Ergebnis√ºbersicht",
    page_icon="üìã",
    layout="wide"
)

# ============================================
# TITLE
# ============================================

st.title("üìã Phase 6: Ergebnis√ºbersicht & Handlungsempfehlungen")
st.markdown("**Professionelles Reporting mit Key Findings und evidenzbasierten Interventionsvorschl√§gen**")

st.divider()

# ============================================
# INFO
# ============================================

st.info("""
**Diese Seite erstellt einen vollst√§ndigen Ergebnis-Report:**
- üìä Deskriptive Statistiken
- üîó Korrelationsanalyse mit Effektst√§rken
- üó∫Ô∏è Quadranten-Analyse (MATHEFF vs ANXMAT)
- üìå Key Findings (dynamisch generiert)
- üí° Handlungsempfehlungen (evidenzbasiert)
- üì• Export-Funktionen (Excel, CSV)
""")

# ============================================
# VARIABLE SELECTION
# ============================================

st.header("1Ô∏è‚É£ Variablen-Auswahl")

available_scales = sorted(list(SCALE_DESCRIPTIONS.keys()))

col1, col2 = st.columns([2, 1])

with col1:
    # Pre-select important variables
    default_vars = ['ANXMAT', 'MATHEFF', 'BELONG', 'ESCS', 'HOMEPOS']
    default_vars = [v for v in default_vars if v in available_scales]

    selected_vars = st.multiselect(
        "WLE-Skalen f√ºr Analyse:",
        options=available_scales,
        default=default_vars,
        help="W√§hle 3-10 Variablen f√ºr umfassende Analyse"
    )

with col2:
    performance_var = st.selectbox(
        "Leistungsvariable:",
        options=['PV1MATH', 'PV1READ', 'PV1SCIE'],
        format_func=lambda x: {
            'PV1MATH': 'Mathematik',
            'PV1READ': 'Lesen',
            'PV1SCIE': 'Naturwissenschaften'
        }[x],
        index=0
    )

# ============================================
# DATA LOADING
# ============================================

@st.cache_data(ttl=60)
def load_report_data(variables, perf_var):
    """Load data for report generation"""
    conn = get_db_connection()

    all_vars = list(set(variables + [perf_var, 'ST004D01T']))
    var_str = ', '.join(all_vars)

    query = f"""
    SELECT {var_str}
    FROM student_data
    WHERE {perf_var} IS NOT NULL
    """

    df = pd.read_sql_query(query, conn)
    conn.close()

    # Rename for easier access
    df.rename(columns={
        perf_var: 'performance',
        'ST004D01T': 'gender'
    }, inplace=True)

    return df


if len(selected_vars) >= 2:

    # Load data
    df = load_report_data(selected_vars, performance_var)

    st.success(f"‚úÖ Daten geladen: N = {len(df):,} Sch√ºler")

    st.divider()

    # ============================================
    # SECTION 1: √úBERSICHT
    # ============================================

    st.header("2Ô∏è‚É£ Stichproben-√úbersicht")

    col1, col2, col3, col4 = st.columns(4)

    with col1:
        st.metric("üë®‚Äçüéì Sch√ºler (gesamt)", f"{len(df):,}")

    with col2:
        female_count = (df['gender'] == 1).sum()
        st.metric("üë© Weiblich", f"{female_count:,}")

    with col3:
        male_count = (df['gender'] == 2).sum()
        st.metric("üë® M√§nnlich", f"{male_count:,}")

    with col4:
        mean_perf = df['performance'].mean()
        st.metric("√ò Leistung", f"{mean_perf:.0f}")

    st.divider()

    # ============================================
    # SECTION 2: DESKRIPTIVE STATISTIK
    # ============================================

    st.header("3Ô∏è‚É£ Deskriptive Statistiken")

    desc_data = []

    for var in selected_vars:
        var_info = get_scale_info(var)

        desc_data.append({
            'Variable': var,
            'Bezeichnung': var_info.get('name_de', var),
            'N': df[var].notna().sum(),
            'Mean': df[var].mean(),
            'SD': df[var].std(),
            'Min': df[var].min(),
            'Max': df[var].max(),
            'Missing': df[var].isna().sum(),
            'Missing %': (df[var].isna().sum() / len(df) * 100)
        })

    # Add performance variable
    desc_data.append({
        'Variable': performance_var,
        'Bezeichnung': {
            'PV1MATH': 'Mathematik-Leistung',
            'PV1READ': 'Lese-Leistung',
            'PV1SCIE': 'Naturwiss.-Leistung'
        }[performance_var],
        'N': df['performance'].notna().sum(),
        'Mean': df['performance'].mean(),
        'SD': df['performance'].std(),
        'Min': df['performance'].min(),
        'Max': df['performance'].max(),
        'Missing': df['performance'].isna().sum(),
        'Missing %': (df['performance'].isna().sum() / len(df) * 100)
    })

    desc_df = pd.DataFrame(desc_data)

    # Format for display
    desc_df_display = desc_df.copy()
    desc_df_display['Mean'] = desc_df_display['Mean'].apply(lambda x: f"{x:.3f}")
    desc_df_display['SD'] = desc_df_display['SD'].apply(lambda x: f"{x:.3f}")
    desc_df_display['Min'] = desc_df_display['Min'].apply(lambda x: f"{x:.2f}")
    desc_df_display['Max'] = desc_df_display['Max'].apply(lambda x: f"{x:.2f}")
    desc_df_display['Missing %'] = desc_df_display['Missing %'].apply(lambda x: f"{x:.1f}%")

    st.dataframe(desc_df_display, use_container_width=True, hide_index=True)

    st.divider()

    # ============================================
    # SECTION 3: KORRELATIONEN
    # ============================================

    st.header("4Ô∏è‚É£ Korrelationen mit Leistung")

    st.markdown(f"**Korrelationen aller Variablen mit {performance_var}:**")

    # Calculate correlations
    corr_data = []

    for var in selected_vars:
        # Remove NaN
        clean_df = df[[var, 'performance']].dropna()

        if len(clean_df) >= 3:
            corr, p_val = correlation_with_pvalue(clean_df[var], clean_df['performance'])
            r2 = corr ** 2

            # Effect size classification (Cohen 1988)
            if abs(corr) < 0.1:
                effect_size = "Sehr klein"
            elif abs(corr) < 0.3:
                effect_size = "Klein"
            elif abs(corr) < 0.5:
                effect_size = "Mittel"
            else:
                effect_size = "Gro√ü"

            var_info = get_scale_info(var)

            corr_data.append({
                'Variable': var,
                'Bezeichnung': var_info.get('name_de', var),
                'r': corr,
                'r (absolut)': abs(corr),
                'R¬≤': r2,
                'R¬≤ (%)': r2 * 100,
                'p-Wert': p_val,
                'Signifikant': 'Ja' if p_val < 0.05 else 'Nein',
                'Effektst√§rke': effect_size,
                'Richtung': 'Positiv' if corr > 0 else 'Negativ'
            })

    corr_df = pd.DataFrame(corr_data).sort_values('r (absolut)', ascending=False)

    # Format for display
    corr_df_display = corr_df.copy()
    corr_df_display['r'] = corr_df_display['r'].apply(lambda x: f"{x:.3f}")
    corr_df_display['r (absolut)'] = corr_df_display['r (absolut)'].apply(lambda x: f"{x:.3f}")
    corr_df_display['R¬≤'] = corr_df_display['R¬≤'].apply(lambda x: f"{x:.3f}")
    corr_df_display['R¬≤ (%)'] = corr_df_display['R¬≤ (%)'].apply(lambda x: f"{x:.1f}%")
    corr_df_display['p-Wert'] = corr_df_display['p-Wert'].apply(lambda x: f"{x:.4f}")

    st.dataframe(corr_df_display, use_container_width=True, hide_index=True)

    # Highlight strongest predictor
    if len(corr_df) > 0:
        strongest = corr_df.iloc[0]
        st.success(f"""
        **üèÜ St√§rkster Pr√§diktor:** {strongest['Bezeichnung']} ({strongest['Variable']})
        - Korrelation: r = {strongest['r']:.3f}
        - Varianzaufkl√§rung: R¬≤ = {strongest['R¬≤ (%)']:.1f}%
        - Effektst√§rke: {strongest['Effektst√§rke']}
        - Richtung: {strongest['Richtung']}
        """)

    st.divider()

    # ============================================
    # SECTION 4: QUADRANTEN-ANALYSE
    # ============================================

    if 'ANXMAT' in selected_vars and 'MATHEFF' in selected_vars:
        st.header("5Ô∏è‚É£ Quadranten-Analyse: MATHEFF vs ANXMAT")

        st.info("""
        **Quadranten-Modell:**
        - **Q1 (Optimal):** Hohe Selbstwirksamkeit + Niedrige Angst ‚Üí Beste Leistung
        - **Q2 (Ambivalent):** Hohe Selbstwirksamkeit + Hohe Angst ‚Üí Gemischt
        - **Q3 (Risikogruppe):** Niedrige Selbstwirksamkeit + Hohe Angst ‚Üí Schlechteste Leistung
        - **Q4 (Indifferent):** Niedrige Selbstwirksamkeit + Niedrige Angst ‚Üí Geringes Engagement
        """)

        # Calculate quadrants
        median_matheff = df['MATHEFF'].median()
        median_anxmat = df['ANXMAT'].median()

        df['quadrant'] = 'Q4'
        df.loc[(df['MATHEFF'] >= median_matheff) & (df['ANXMAT'] < median_anxmat), 'quadrant'] = 'Q1'
        df.loc[(df['MATHEFF'] >= median_matheff) & (df['ANXMAT'] >= median_anxmat), 'quadrant'] = 'Q2'
        df.loc[(df['MATHEFF'] < median_matheff) & (df['ANXMAT'] >= median_anxmat), 'quadrant'] = 'Q3'
        df.loc[(df['MATHEFF'] < median_matheff) & (df['ANXMAT'] < median_anxmat), 'quadrant'] = 'Q4'

        # Statistics per quadrant
        quadrant_stats = df.groupby('quadrant').agg({
            'performance': ['mean', 'std', 'count']
        }).round(2)
        quadrant_stats.columns = ['√ò Leistung', 'SD Leistung', 'N']
        quadrant_stats['Anteil %'] = (quadrant_stats['N'] / len(df) * 100).round(1)

        # Relabel quadrants
        quadrant_stats.index = quadrant_stats.index.map({
            'Q1': 'Q1: Optimal (Hoch/Niedrig)',
            'Q2': 'Q2: Ambivalent (Hoch/Hoch)',
            'Q3': 'Q3: Risikogruppe (Niedrig/Hoch)',
            'Q4': 'Q4: Indifferent (Niedrig/Niedrig)'
        })

        st.dataframe(quadrant_stats, use_container_width=True)

        # Highlight risk group
        q3_n = quadrant_stats.loc['Q3: Risikogruppe (Niedrig/Hoch)', 'N']
        q3_pct = quadrant_stats.loc['Q3: Risikogruppe (Niedrig/Hoch)', 'Anteil %']
        q3_perf = quadrant_stats.loc['Q3: Risikogruppe (Niedrig/Hoch)', '√ò Leistung']

        st.warning(f"""
        **‚ö†Ô∏è Risikogruppe (Q3):**
        - **{q3_n:.0f} Sch√ºler** ({q3_pct:.1f}% der Stichprobe)
        - **Durchschnittsleistung:** {q3_perf:.0f} Punkte
        - **Intervention dringend empfohlen:** Fokus auf Selbstwirksamkeitsf√∂rderung und Angstreduktion
        """)

        # Optimal group
        q1_n = quadrant_stats.loc['Q1: Optimal (Hoch/Niedrig)', 'N']
        q1_pct = quadrant_stats.loc['Q1: Optimal (Hoch/Niedrig)', 'Anteil %']
        q1_perf = quadrant_stats.loc['Q1: Optimal (Hoch/Niedrig)', '√ò Leistung']

        st.success(f"""
        **‚úÖ Optimale Gruppe (Q1):**
        - **{q1_n:.0f} Sch√ºler** ({q1_pct:.1f}% der Stichprobe)
        - **Durchschnittsleistung:** {q1_perf:.0f} Punkte
        - **Status:** F√∂rderung aufrechterhalten
        """)

        st.divider()

    # ============================================
    # SECTION 5: KEY FINDINGS
    # ============================================

    st.header("6Ô∏è‚É£ Key Findings")

    st.markdown("**üìå Zusammenfassung der wichtigsten Erkenntnisse:**")

    # Dynamically generated key findings
    findings = []

    # Finding 1: Sample
    findings.append(f"**Stichprobe:** N = {len(df):,} Sch√ºler aus PISA 2022 Deutschland")

    # Finding 2: Strongest predictor
    if len(corr_df) > 0:
        strongest = corr_df.iloc[0]
        findings.append(
            f"**St√§rkster Pr√§diktor:** {strongest['Bezeichnung']} mit r = {strongest['r']:.3f} "
            f"(R¬≤ = {strongest['R¬≤ (%)']:.1f}%, Effektst√§rke: {strongest['Effektst√§rke']})"
        )

    # Finding 3: MATHEFF vs ANXMAT
    if 'ANXMAT' in selected_vars and 'MATHEFF' in selected_vars:
        corr_matheff = corr_df[corr_df['Variable'] == 'MATHEFF']['r'].values[0]
        corr_anxmat = corr_df[corr_df['Variable'] == 'ANXMAT']['r'].values[0]

        if float(corr_anxmat) != 0:
            ratio = abs(float(corr_matheff) / float(corr_anxmat))

            findings.append(
                f"**Selbstwirksamkeit vs. Angst:** MATHEFF (r = {float(corr_matheff):.3f}) ist "
                f"{ratio:.2f}x einflussreicher als ANXMAT (r = {float(corr_anxmat):.3f})"
            )

        # Finding 4: Risk group
        if 'quadrant_stats' in locals():
            findings.append(
                f"**Risikogruppe:** {q3_n:.0f} Sch√ºler ({q3_pct:.1f}%) mit niedriger "
                f"Selbstwirksamkeit UND hoher Angst ‚Üí Priorit√§t f√ºr Interventionen"
            )

    # Finding 5: Average performance
    mean_perf = df['performance'].mean()
    if mean_perf < 482:
        pisa_level = "Level 2 (Basiskompetenzen)"
    elif mean_perf < 545:
        pisa_level = "Level 3 (Solide Kenntnisse)"
    elif mean_perf < 607:
        pisa_level = "Level 4 (Gut)"
    else:
        pisa_level = "Level 5+ (Sehr gut)"

    findings.append(
        f"**Durchschnittsleistung:** {mean_perf:.0f} PISA-Punkte ‚Üí {pisa_level}"
    )

    # Output findings
    for i, finding in enumerate(findings, 1):
        st.markdown(f"{i}. {finding}")

    st.divider()

    # ============================================
    # SECTION 6: HANDLUNGSEMPFEHLUNGEN
    # ============================================

    st.header("7Ô∏è‚É£ Handlungsempfehlungen")

    st.markdown("**üí° Evidenzbasierte Interventionsvorschl√§ge:**")

    recommendations = []

    # MATHEFF-based recommendations
    if 'MATHEFF' in selected_vars:
        corr_matheff_val = float(corr_df[corr_df['Variable'] == 'MATHEFF']['r'].values[0])
        if abs(corr_matheff_val) > 0.5:
            recommendations.append({
                'Priorit√§t': 'üî¥ Hoch',
                'Bereich': 'Selbstwirksamkeitsf√∂rderung',
                'Ma√ünahme': 'Mastery Experiences: Strukturierte Erfolgserlebnisse schaffen (z.B. gestufte Aufgaben)',
                'Begr√ºndung': f'Starke Korrelation (r = {corr_matheff_val:.3f}) rechtfertigt hohen Fokus',
                'Theorie': 'Bandura (1997): Self-Efficacy Theory'
            })

    # ANXMAT-based recommendations
    if 'ANXMAT' in selected_vars:
        corr_anxmat_val = float(corr_df[corr_df['Variable'] == 'ANXMAT']['r'].values[0])
        if abs(corr_anxmat_val) > 0.3:
            recommendations.append({
                'Priorit√§t': 'üü° Mittel',
                'Bereich': 'Angstreduktion',
                'Ma√ünahme': 'Kognitive Umstrukturierung & Entspannungstechniken (z.B. progressive Muskelrelaxation)',
                'Begr√ºndung': f'Mittlere Korrelation (r = {corr_anxmat_val:.3f})',
                'Theorie': 'Beck (1976): Cognitive Therapy'
            })

    # Risk group intervention
    if 'ANXMAT' in selected_vars and 'MATHEFF' in selected_vars and 'quadrant_stats' in locals():
        if q3_pct > 15:
            recommendations.append({
                'Priorit√§t': 'üî¥ Hoch',
                'Bereich': 'Risikogruppen-Intervention',
                'Ma√ünahme': 'Individuelle F√∂rderung f√ºr Q3-Sch√ºler: Kombination aus Selbstwirksamkeits-Training und Angstbew√§ltigung',
                'Begr√ºndung': f'{q3_pct:.1f}% der Sch√ºler in kritischer Konstellation (niedrige SE + hohe Angst)',
                'Theorie': 'Differentielle Intervention nach Bedarf'
            })

    # General recommendations
    recommendations.append({
        'Priorit√§t': 'üü¢ Basis',
        'Bereich': 'Diagnostik & Monitoring',
        'Ma√ünahme': 'Regelm√§√üiges Screening von Selbstwirksamkeit & Angst (z.B. quartalsweise)',
        'Begr√ºndung': 'Fr√ºherkennung erm√∂glicht rechtzeitige Intervention',
        'Theorie': 'Response to Intervention (RTI) Framework'
    })

    recommendations.append({
        'Priorit√§t': 'üü¢ Basis',
        'Bereich': 'Lehrkr√§ftefortbildung',
        'Ma√ünahme': 'Professionalisierung im Bereich motivationale √úberzeugungen und Emotionsregulation',
        'Begr√ºndung': 'Lehrkr√§fte ben√∂tigen Wissen √ºber psychologische Faktoren',
        'Theorie': 'Professional Development Research'
    })

    rec_df = pd.DataFrame(recommendations)
    st.dataframe(rec_df, use_container_width=True, hide_index=True)

    st.divider()

    # ============================================
    # SECTION 7: EXPORT
    # ============================================

    st.header("8Ô∏è‚É£ Ergebnisse exportieren")

    st.markdown("""
    **Lade alle Ergebnisse herunter f√ºr:**
    - Weiterverarbeitung in Excel/SPSS
    - Integration in Berichte und Pr√§sentationen
    - Dokumentation und Archivierung
    """)

    col1, col2, col3 = st.columns(3)

    with col1:
        st.markdown("### üìä Deskriptive Statistiken")

        csv_desc = desc_df.to_csv(index=False, encoding='utf-8-sig')
        st.download_button(
            label="üì• CSV herunterladen",
            data=csv_desc,
            file_name=f"pisa_deskriptiv_{pd.Timestamp.now().strftime('%Y%m%d')}.csv",
            mime="text/csv",
            key="download_desc"
        )

    with col2:
        st.markdown("### üîó Korrelationen")

        csv_corr = corr_df.to_csv(index=False, encoding='utf-8-sig')
        st.download_button(
            label="üì• CSV herunterladen",
            data=csv_corr,
            file_name=f"pisa_korrelationen_{pd.Timestamp.now().strftime('%Y%m%d')}.csv",
            mime="text/csv",
            key="download_corr"
        )

    with col3:
        if 'quadrant_stats' in locals():
            st.markdown("### üó∫Ô∏è Quadranten")

            csv_quad = quadrant_stats.to_csv(encoding='utf-8-sig')
            st.download_button(
                label="üì• CSV herunterladen",
                data=csv_quad,
                file_name=f"pisa_quadranten_{pd.Timestamp.now().strftime('%Y%m%d')}.csv",
                mime="text/csv",
                key="download_quad"
            )

    st.divider()

    # EXCEL Export with all sheets
    st.markdown("### üìó Kompletter Export (Excel mit allen Sheets)")

    try:
        # Create Excel file in memory
        output = BytesIO()
        with pd.ExcelWriter(output, engine='openpyxl') as writer:
            # Sheet 1: Overview
            overview_data = {
                'Kennzahl': [
                    'Analysedatum',
                    'Leistungsvariable',
                    'Anzahl Sch√ºler',
                    'Durchschnittsleistung',
                    'PISA-Level',
                    'Anzahl analysierte Variablen'
                ],
                'Wert': [
                    pd.Timestamp.now().strftime('%Y-%m-%d'),
                    performance_var,
                    len(df),
                    f"{mean_perf:.2f}",
                    pisa_level,
                    len(selected_vars)
                ]
            }
            pd.DataFrame(overview_data).to_excel(writer, sheet_name='√úbersicht', index=False)

            # Sheet 2: Descriptive Statistics
            desc_df.to_excel(writer, sheet_name='Deskriptive Statistik', index=False)

            # Sheet 3: Correlations
            corr_df.to_excel(writer, sheet_name='Korrelationen', index=False)

            # Sheet 4: Quadrants (if available)
            if 'quadrant_stats' in locals():
                quadrant_stats.to_excel(writer, sheet_name='Quadranten-Analyse')

            # Sheet 5: Recommendations
            rec_df.to_excel(writer, sheet_name='Handlungsempfehlungen', index=False)

        output.seek(0)

        st.download_button(
            label="üì• Excel-Report herunterladen",
            data=output,
            file_name=f"PISA_Ergebnisreport_{pd.Timestamp.now().strftime('%Y%m%d')}.xlsx",
            mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
            use_container_width=True
        )

        st.success("‚úÖ Excel-Report bereit zum Download!")

    except Exception as e:
        st.error(f"‚ùå Fehler beim Erstellen des Excel-Reports: {e}")
        st.info("üí° Tipp: Installiere openpyxl mit `pip install openpyxl`")

else:
    st.warning("‚ö†Ô∏è Bitte w√§hle mindestens 2 Variablen f√ºr die Analyse.")

# ============================================
# FOOTER
# ============================================

st.divider()

st.caption("""
üìã **Ergebnis√ºbersicht** | PISA 2022 Deutschland |
Evidenzbasierte Handlungsempfehlungen nach wissenschaftlichen Standards
""")
